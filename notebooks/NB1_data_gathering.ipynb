{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Google Trends & Stock Prices: Data Gathering\n",
      "---------------------------------------\n",
      "We will create a python file \"data_gathering.py\" that aggregates two types of data:\n",
      "\n",
      "1. Google Trends values from Google Trends\n",
      "2. Stock prices from Yahoo Finance API\n",
      "\n",
      "Our goal is to find a relationship between the Google Trends values and the stock prices for the companies in the Dow Jones Index, a list of U.S.'s 30 top companies. The end product of the script should be many different csv files with three columns: \"Date\", \"Google Trends Value\", and \"Stock Price\" for different companies. Excited? Great, then let's begin!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preliminary Setup\n",
      "-----------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #0: Contributors\n",
      "* Anshuman Dewangan: Researched all sources for data gathering. Wrote all data gathering code. Authored data gathering ipython notebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #1: Creating the Directory Structure\n",
      "First, let's set up our directory structure according to the directions given. We'll create three folders in our current directory: script, data, and visualizations. Inside data, we'll create three more directories: raw, cleaned, and simulated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "# Makes the directories and displays the contents of the current directory\n",
      "mkdir ./script ./data ./data/raw ./data/cleaned ./data/simulated ./visualizations\n",
      "ls -r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "visualizations\n",
        "script\n",
        "desktop.ini\n",
        "data\n",
        "README.md\n",
        "NB4_project_report.ipynb\n",
        "NB3_data_analysis.ipynb\n",
        "NB2_data_cleaning.ipynb\n",
        "NB1_data_gathering.ipynb\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "mkdir: cannot create directory `./script': File exists\n",
        "mkdir: cannot create directory `./data': File exists\n",
        "mkdir: cannot create directory `./data/raw': File exists\n",
        "mkdir: cannot create directory `./data/cleaned': File exists\n",
        "mkdir: cannot create directory `./data/simulated': File exists\n",
        "mkdir: cannot create directory `./visualizations': File exists\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Gathering Code\n",
      "----------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step #2: Module Import\n",
      "As a first step in our code, let's make all the necessary imports that we will need to use. We'll see how each import is being used later on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports all important modules for the program\n",
      "import sys\n",
      "import datetime\n",
      "import pandas\n",
      "import csv\n",
      "import cookielib\n",
      "import mechanize\n",
      "import re\n",
      "from StringIO import StringIO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #3: Load Dow Jones Data\n",
      "Next, let's load the file \"DowJones_Modified.csv\" into a pandas dataframe called \"stocks\". \"DowJones_Modified.csv\" is a manually created file of a list of companies in the  Dow Jones Index from Yahoo Finance with two columns: \"Company\", \"Ticker\". However, we modified the company names to incorporate more popular search terms. For example, we would use \"American Express\" instead of \"American Express Company\" to get more search results.\n",
      "\n",
      "Source: http://finance.yahoo.com/q/cp?s=%5EDJI"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reads \"DowJones_Modified.csv\" of stocks and ticker symbols into a pandas dataframe\n",
      "stocks = pandas.read_csv('./data/DowJones_Modified.csv')\n",
      "companies = stocks.Company\n",
      "tickers = stocks.Ticker\n",
      "\n",
      "print stocks[0:6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "            Company Ticker\n",
        "0  American Express    AXP\n",
        "1            Boeing     BA\n",
        "2       Caterpillar    CAT\n",
        "3             Cisco   CSCO\n",
        "4           Chevron    CVX\n",
        "5           du Pont     DD\n",
        "\n",
        "[6 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #4: Create Queries Lists\n",
      "Now we will create a list of lists with all the queries we want to search on Google Trends. We want to test four different queries for each company:\n",
      "\n",
      "1. Company name\n",
      "2. Ticker symbol\n",
      "3. \"Buy\" + ticker symbol\n",
      "4. \"Sell\" + ticker symbol\n",
      "\n",
      "For example, Apple will have four different queries:\n",
      "\n",
      "1. Apple\n",
      "2. AAPL\n",
      "3. Buy AAPL\n",
      "4. Sell AAPL\n",
      "\n",
      "We'll create a class that takes in different types of queries for each company. The input will be the following arguments: \"company\", \"ticker\", \"'buy ' + ticker\", \"'sell ' + ticker\". The output will be a list of lists with all the search terms we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creates a list of lists with search terms for all companies in the format given by inputs\n",
      "def query(*arg):\n",
      "    queries = []\n",
      "    for query in arg:\n",
      "        queries.append([query[n] for n in range(0, len(stocks))])\n",
      "    return queries\n",
      "        \n",
      "queries = query(companies, tickers, \"buy \" + tickers, \"sell \" + tickers)\n",
      "print queries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['American Express', 'Boeing', 'Caterpillar', 'Cisco', 'Chevron', 'du Pont', 'Disney', 'General Electric', 'Goldman Sachs', 'Home Depot', 'IBM', 'Intel', 'Johnson & Johnson', 'JPMorgan Chase', 'Coca-Cola', \"McDonald's\", '3M', 'Merck', 'Microsoft', 'Nike', 'Pfizer', 'Procter & Gamble', 'AT&T', 'Travelers', 'UnitedHealth', 'United Technologies', 'Visa', 'Verizon', 'WalMart', 'Exxon Mobil'], ['AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE', 'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'T', 'TRV', 'UNH', 'UTX', 'V', 'VZ', 'WMT', 'XOM'], ['buy AXP', 'buy BA', 'buy CAT', 'buy CSCO', 'buy CVX', 'buy DD', 'buy DIS', 'buy GE', 'buy GS', 'buy HD', 'buy IBM', 'buy INTC', 'buy JNJ', 'buy JPM', 'buy KO', 'buy MCD', 'buy MMM', 'buy MRK', 'buy MSFT', 'buy NKE', 'buy PFE', 'buy PG', 'buy T', 'buy TRV', 'buy UNH', 'buy UTX', 'buy V', 'buy VZ', 'buy WMT', 'buy XOM'], ['sell AXP', 'sell BA', 'sell CAT', 'sell CSCO', 'sell CVX', 'sell DD', 'sell DIS', 'sell GE', 'sell GS', 'sell HD', 'sell IBM', 'sell INTC', 'sell JNJ', 'sell JPM', 'sell KO', 'sell MCD', 'sell MMM', 'sell MRK', 'sell MSFT', 'sell NKE', 'sell PFE', 'sell PG', 'sell T', 'sell TRV', 'sell UNH', 'sell UTX', 'sell V', 'sell VZ', 'sell WMT', 'sell XOM']]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #5: Setup Browser\n",
      "Before we can actually pull the data from Google Trends, however, we run into two problems:\n",
      "\n",
      "1. The data is only securely available to Google users\n",
      "2. The data is not directly downloadable from a csv url\n",
      "\n",
      "Therefore, we have create a virtual browser that would allow us to pull csv files directly by authenticating Google using login credentials. Otherwise, there is no direct way to download the csv data through programming code. For ideas on how to do this, we got some major help from our source below.\n",
      "\n",
      "Source: https://gist.github.com/tlmaloney/5650699"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Open browser\n",
      "br = mechanize.Browser()\n",
      "\n",
      "# Make it seem like we are an actual person\n",
      "br.set_handle_robots(False)\n",
      "\n",
      "# Create cookie jar\n",
      "cj = cookielib.LWPCookieJar()\n",
      "br.set_cookiejar(cj)\n",
      "\n",
      "# Act like we're a real browser\n",
      "br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "\n",
      "# Login in to Google\n",
      "response = br.open('https://accounts.google.com/ServiceLogin?hl=en&continue=https://www.google.com/')\n",
      "forms = mechanize.ParseResponse(response)\n",
      "form = forms[0]\n",
      "form['Email'] = \"DherintoR\"\n",
      "form['Passwd'] = \"forthestars\"\n",
      "response = br.open(form.click())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #6: Get Google Trends Data\n",
      "Ok, now we are ready to get Google Trends data for one of our queries: American Express Company. First, we'll put our queries, which are often more than one word, into a form the browser will understand: with \"%20\" between each word. Then we will append the search to our Google Trends url to get a csv of the Google Trends data.\n",
      "\n",
      "Source: http://www.google.com/trends/explore"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split queries list into a format that can be entered into the url       \n",
      "query_split = queries[0][0].split()\n",
      "query_params = '%20'.join(query_split)\n",
      "\n",
      "# Get CSV from Google Trends   \n",
      "response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "reader = csv.reader(StringIO(response.read()))\n",
      "\n",
      "# Print first 10 lines of csv file\n",
      "print queries[0][0]\n",
      "for x in range(0,10):\n",
      "    print reader.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "American Express\n",
        "['Web Search interest: american express']\n",
        "['Worldwide; 2004 - present']\n",
        "[]\n",
        "['Interest over time']\n",
        "['Week', 'american express']\n",
        "['2004-01-04 - 2004-01-10', '73']\n",
        "['2004-01-11 - 2004-01-17', '73']\n",
        "['2004-01-18 - 2004-01-24', '72']\n",
        "['2004-01-25 - 2004-01-31', '65']\n",
        "['2004-02-01 - 2004-02-07', '70']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step #7: Clean the Data\n",
      "Interesting, the csv file comes with a lot of junk information. Additionally, the actual Google Trends data comes in an awkward format: \"Date (given as a weekly range), \"Google Trends Value\". Let's clean the data so that it takes the date for the Friday of each week and puts all the information into two different lists: \"dates\" and \"values\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reload the page\n",
      "response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "reader = csv.reader(StringIO(response.read()))\n",
      "\n",
      "# Create empty lists to be populated with date and trend values\n",
      "dates = []\n",
      "values = []\n",
      "\n",
      "# Populate lists with dates and Google Trends values in the correct formate.\n",
      "for row in reader:\n",
      "    # Looks for the right rows to get data from, ignoring the junk\n",
      "    try:\n",
      "        date, value = row\n",
      "    except ValueError:\n",
      "        continue\n",
      "    # Takes the date for the Friday of the week and puts it in the correct format\n",
      "    if re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', date):\n",
      "        dates.append(str((datetime.datetime.strptime(date[-10:], \"%Y-%m-%d\")-datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\"))) # Uses last date in time period\n",
      "        values.append(value)\n",
      "\n",
      "#Print the contents of the list\n",
      "print dates[0:6]\n",
      "print values[0:6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['2004-01-09', '2004-01-16', '2004-01-23', '2004-01-30', '2004-02-06', '2004-02-13']\n",
        "['73', '73', '72', '65', '70', '70']\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #8: Get Stock Data\n",
      "Great! We have two lists, one with the dates and one with the corresponding Google Trends values. Now let's try getting stock data on the ticker symbol for American Express (AXP) using the Yahoo Finance API. We'll load a csv with daily stock price data of American Express (AXP) from January 1, 2004 (when Google Trends data begins) to present.\n",
      "\n",
      "Source: https://code.google.com/p/yahoo-finance-managed/wiki/csvHistQuotesDownload"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a url that acceses Yahoo Finance historical data from 01-01-2004 to present for tickers[0] = JNJ\n",
      "base_url = \"http://ichart.yahoo.com/table.csv?s=\"\n",
      "dt_url = '%s&a=%s&b=%s&c=%s&d=%s&e=%s&f=%s&g=d&ignore=.csv'% (tickers[0], \"01\", \"01\", \"2004\", \"00\", \"00\", \"0000\")\n",
      "\n",
      "# Opens the Yahoo Finance csv file\n",
      "response1 = br.open(base_url + dt_url)\n",
      "reader1 = csv.reader(StringIO(response1.read()))\n",
      "\n",
      "# Print first 6 lines of csv file\n",
      "print tickers[0]\n",
      "for x in range(0,6):\n",
      "    print reader1.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AXP\n",
        "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "['2014-04-29', '87.34', '87.95', '87.19', '87.79', '2852900', '87.79']\n",
        "['2014-04-28', '87.44', '87.58', '85.85', '87.06', '4760400', '87.06']\n",
        "['2014-04-25', '87.02', '87.30', '86.62', '87.03', '4226100', '87.03']\n",
        "['2014-04-24', '87.26', '87.98', '86.91', '87.41', '3187000', '87.41']\n",
        "['2014-04-23', '87.01', '87.64', '86.67', '87.13', '3788200', '87.13']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #9: Add Prices\n",
      "Next, we'll save the closing prices of the stock data in a dictionary called \"prices_dict\". Then, we will use this dictionary to add prices to the list \"prices\" for the corresponding date in \"dates\". If no price is found for a specific date, we will enter \"\"; these values can be removed later during data cleaning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reloads the csv file\n",
      "response1 = br.open(base_url + dt_url)\n",
      "reader1 = csv.reader(StringIO(response1.read()))\n",
      "\n",
      "# Creates a dictionary that maps dates to closing prices for that day\n",
      "prices_dict = {row[0]: row[4] for row in reader1}\n",
      "\n",
      "# Adds stock prices to the corresponding dates found in the Google Trends data\n",
      "prices = []\n",
      "for date in dates:\n",
      "    # Tries to add the price for the date from the Google Trends data\n",
      "    try:\n",
      "        prices.append(prices_dict[date])\n",
      "    # Adds a blank value if the price is not available for the Google Trends date\n",
      "    except:\n",
      "        prices.append(\"\")\n",
      "        \n",
      "# Print the contents of \"prices\" list\n",
      "print prices[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['', '', '', '', '53.06', '53.65', '53.16', '53.42', '52.77', '52.12', '51.08', '50.03', '52.43', '', '50.58', '50.27', '48.95', '48.49', '48.86', '49.63']\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step #10: Write the csv file\n",
      "Fantastic! Now we have three lists:\n",
      "\n",
      "1. dates - Dates\n",
      "2. values - Google Trends values\n",
      "3. prices - Stock prices\n",
      "\n",
      "The last step is to write a csv file containing all these values in separate columns: \"Date\", \"Value\" which is denoted by the query term, and \"Price\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets the filename as the query\n",
      "filename = queries[0][0]\n",
      "filename.replace(\" \", \"\")\n",
      "\n",
      "# Creates a file and writes values from our three lists\n",
      "if dates != []:\n",
      "    with open(\"./data/raw/DowJones_Modified/\" + filename + \"_gtrends.csv\", 'w') as file:\n",
      "        writer = csv.writer(file, lineterminator = '\\n')\n",
      "        # Writes a header row with the titles: Date, Query, Price\n",
      "        writer.writerow(['Date', queries[0][0], \"Price\"])\n",
      "        # Writes a row for each value of date, value, and price\n",
      "        for row in zip(dates, values, prices):\n",
      "            writer.writerow(row)\n",
      "        # Closes and saves the file\n",
      "        file.close()\n",
      "else:\n",
      "    print \"Failed to get data for \" + queries[x][n]\n",
      "    \n",
      "# View contents of csv file\n",
      "with open(\"./data/raw/DowJones_Modified/\" + filename + \"_gtrends.csv\", 'r') as file:\n",
      "    for x in range(0,10):\n",
      "        print file.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Date,American Express,Price\n",
        "\n",
        "2004-01-09,73,\n",
        "\n",
        "2004-01-16,73,\n",
        "\n",
        "2004-01-23,72,\n",
        "\n",
        "2004-01-30,65,\n",
        "\n",
        "2004-02-06,70,53.06\n",
        "\n",
        "2004-02-13,70,53.65\n",
        "\n",
        "2004-02-20,79,53.16\n",
        "\n",
        "2004-02-27,69,53.42\n",
        "\n",
        "2004-03-05,79,52.77\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Final Script: data_gathering.py\n",
      "Awesome! Now we can write a full script \"data_gathering.py\" that automates the process for all search queries. The result is many different csv files created for each query. These csv files can easily be read into R dataframes, as seen in the data cleaning step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file script/data_gathering.py\n",
      "\n",
      "\"\"\"\n",
      "Downloads Google Trends and stock data for search terms related to companies in the Dow Jones Index. \n",
      "\n",
      "Creates a csv file with \"Date\", \"Google Trends value\", and \"Price\" for each search term.\n",
      "\n",
      "Files will be saved in the \"../data/raw/\" directory.\n",
      "\"\"\"\n",
      "\n",
      "# Imports all important modules for the program\n",
      "import sys\n",
      "import datetime\n",
      "import pandas\n",
      "import csv\n",
      "import cookielib\n",
      "import mechanize\n",
      "import re\n",
      "from StringIO import StringIO\n",
      " \n",
      "# Reads \"DowJones_Modified.csv\" of stocks and ticker symbols into a pandas dataframe\n",
      "def stocks_data(data='../data/DowJones_Modified.csv'):\n",
      "    stocks = pandas.read_csv(data)\n",
      "    company = stocks.Company\n",
      "    ticker = stocks.Ticker\n",
      "    return stocks, company, ticker\n",
      "stocks, companies, tickers = stocks_data()\n",
      "\n",
      "\n",
      "# Creates a list of lists with search terms for all companies in the format given by inputs\n",
      "def query(*arg):\n",
      "    queries = []\n",
      "    for query in arg:\n",
      "        queries.append([query[n] for n in range(0, len(stocks))])\n",
      "    return queries       \n",
      "queries = query(companies, tickers, \"buy \" + tickers, \"sell \" + tickers)\n",
      "\n",
      "br = mechanize.Browser()\n",
      "def browser_setup():\n",
      "    # Open browser\n",
      "    br.set_handle_robots(False)\n",
      "    \n",
      "    # Create cookie jar\n",
      "    cj = cookielib.LWPCookieJar()\n",
      "    br.set_cookiejar(cj)\n",
      "    \n",
      "    # Act like we're a real browser\n",
      "    br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "    \n",
      "    # Login in to Google\n",
      "    response = br.open('https://accounts.google.com/ServiceLogin?hl=en&continue=https://www.google.com/')\n",
      "    forms = mechanize.ParseResponse(response)\n",
      "    form = forms[0]\n",
      "    form['Email'] = \"DherintoR\"\n",
      "    form['Passwd'] = \"forthestars\"\n",
      "    response = br.open(form.click())\n",
      "browser_setup()\n",
      "\n",
      "\n",
      "# Pulls Yahoo Finance stock data for given ticker from 2004-present\n",
      "def yahoo_finance(ticker_symbol):\n",
      "    base_url = \"http://ichart.yahoo.com/table.csv?s=\"\n",
      "    dt_url = '%s&a=%s&b=%s&c=%s&d=%s&e=%s&f=%s&g=d&ignore=.csv'% (ticker_symbol, \"01\", \"01\", \"2004\", \"00\", \"00\", \"0000\")\n",
      "    return base_url + dt_url\n",
      "\n",
      "    \n",
      "# Create CSV for each stock in stocks list\n",
      "for n in range(0,len(companies)):\n",
      "    \n",
      "    # Get stock data for the stock\n",
      "    url = yahoo_finance(tickers[n])\n",
      "    # Opens the Yahoo Finance csv file\n",
      "    response1 = br.open(url)\n",
      "    reader1 = csv.reader(StringIO(response1.read()))\n",
      "    # Creates a dictionary that maps dates to closing prices for that day\n",
      "    prices_dict = {row[0]: row[4] for row in reader1}\n",
      "\n",
      "    for x in range(0,len(queries)):\n",
      "        \n",
      "        # Split queries list into a format that can be entered into the url                \n",
      "        query_split = queries[x][n].split()\n",
      "        query_params = '%20'.join(query_split)\n",
      "        \n",
      "        # Get CSV from Google Trends\n",
      "        response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "        reader = csv.reader(StringIO(response.read()))\n",
      "        \n",
      "        # Creates empty lists to be populated with data\n",
      "        dates = []\n",
      "        values = []\n",
      "        prices = []\n",
      "        \n",
      "        # Populate lists with dates and Google Trends values in the correct formate.\n",
      "        for row in reader:\n",
      "            # Looks for the right rows to get data from, ignoring the junk\n",
      "            try:\n",
      "                date, value = row\n",
      "            except ValueError:\n",
      "                continue\n",
      "            # Takes the date for the Friday of the week and puts it in the correct format\n",
      "            if re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', date):\n",
      "                dates.append(str((datetime.datetime.strptime(date[-10:], \"%Y-%m-%d\")-datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\"))) # Uses last date in time period\n",
      "                values.append(value)\n",
      "        \n",
      "        # Adds stock prices to the corresponding dates found in the Google Trends data\n",
      "        for date in dates:\n",
      "            # Tries to add the price for the date from the Google Trends data\n",
      "            try:\n",
      "                prices.append(prices_dict[date])\n",
      "            # Adds a blank value if the price is not available for the Google Trends date\n",
      "            except:\n",
      "                prices.append(\"\")\n",
      "\n",
      "        # Sets the filename as the query\n",
      "        filename = queries[x][n]\n",
      "        filename.replace(\" \", \"\")\n",
      "        \n",
      "        # Creates a file in \"data/raw/DowJones_Modified/\" and writes values from our three lists\n",
      "        if dates != []:\n",
      "            with open(\"../data/raw/DowJones_Modified/\" + filename + \"_gtrends.csv\", 'w') as file:\n",
      "                writer = csv.writer(file, lineterminator = '\\n')\n",
      "                # Writes a header row with the titles: Date, Query, Price\n",
      "                writer.writerow(['Date', queries[x][n], \"Price\"])\n",
      "                # Writes a row for each value of date, value, and price\n",
      "                for row in zip(dates, values, prices):\n",
      "                    writer.writerow(row)\n",
      "                # Closes and saves the file\n",
      "                file.close()\n",
      "        else:\n",
      "            print \"Failed to get data for \" + queries[x][n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting script/data_gathering.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}