{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, you'll\n",
      "\n",
      "* create the directory structure for your project\n",
      "* download the raw data for your project\n",
      "* **write functions and classes to transform raw data (e.g in XML, jason, etc.) into data frames**\n",
      "* explain what your are doing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Team members responsible for this notebook:\n",
      "\n",
      "List the team members contributing to this notebook, along with their responsabilities:\n",
      "\n",
      "* team member 1 **name**: team member 1 **responsabilities**\n",
      "* team member 2 **name**: team member 2 **responsabilities**\n",
      "* etc.\n",
      "\n",
      "I advise you to work at least in pairs for each project notebook, as you did for the homework assignments. Of course, all team members may participate to each notebook. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Instructions:\n",
      "\n",
      "In **markdown cells**, you'll\n",
      "\n",
      "* list and describe the data sources for your class project\n",
      "    * provide the data urls\n",
      "    * describe the file formats\n",
      "    * explain the data content of each file\n",
      "   \n",
      "**Remark:** If your data is not directly available through the web (for instance, you requested it from a company, or got it from Inna or Henry), place you data into your personal bdrive, and provide a link to it, that you'll use in this notebook to download it into the data folder, as explained below).\n",
      "\n",
      "\n",
      "In **code cells**, you'll\n",
      "\n",
      "* create the following directories:\n",
      "\n",
      "        ./data\n",
      "        ./data/raw        # to store your raw data\n",
      "        ./data/cleaned    # to store the cleanned data\n",
      "        ./data/simulated  # to store simulated data\n",
      "        ./visualizations  # to store your plots\n",
      "\n",
      "\n",
      "* display samples of your data files (e.g. for a csv files, use data frames and the <code>head</code> method)\n",
      "\n",
      "\n",
      "*  Write **scripts** into the <code>./script</code> directory using the he **magic command** \n",
      "\n",
      "    %%file ./script/file_name\n",
      "    \n",
      "Your scripts should contain **functions** and **objects** allowing you to\n",
      "\n",
      "* download the actual raw data in the directory <code>./data/raw</code>\n",
      "\n",
      "\n",
      "* load the downloaded the raw data files into data frames\n",
      "\n",
      "This way, the functions and objects contained in your scripts will be accessible in other notebooks through the <code>import</code> command in Python (see example below).\n",
      "\n",
      "\n",
      "**Remark 1:** In the code cells, you may use Python, R, or Bash, as you find more convenient.\n",
      "\n",
      "\n",
      "**Remark 2:** Try to make your notebook as readable and usable (by others) as possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is an example of how to package your code into scripts reusable in other notebook.\n",
      "\n",
      "You'll need to do something similar to that. You'll also need to write explanations as outlined above in markdown cells."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Creating the directory structure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "mkdir ../script ../data ../data/raw ../data/cleaned ../data/simulated ../visualizations\n",
      "ls -r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "visualizations\n",
        "script\n",
        "README.md\n",
        "NB4_project_report.ipynb\n",
        "NB3_data_analysis.ipynb\n",
        "NB2_data_cleaning.ipynb\n",
        "NB1_data_gathering.ipynb\n",
        "data\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "mkdir: cannot create directory \u2018../data\u2019: File exists\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Downloading and displaying raw data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, I am using R. You want to use Python instead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TO DO:** list sources, display data sample, explain data content, etc. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now download an XML source containing plant data, and display an typical plant entry, represented as a XML node:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "library(XML)\n",
      "plant_url  =  'http://www.stat.berkeley.edu/classes/s133/data/plant_catalog.xml'\n",
      "plant_file = './data/raw/plant.xml'\n",
      "\n",
      "download.file(plant_url, plant_file, method=\"curl\")\n",
      "\n",
      "xml_doc   = xmlParse(plant_file)\n",
      "root_node = xmlRoot(xml_doc)\n",
      "\n",
      "plant_nodes = xmlChildren(root_node)\n",
      "print(plant_nodes[[1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\n",
        "Attaching package: \u2018XML\u2019\n",
        "\n",
        "The following object(s) are masked from \u2018package:tools\u2019:\n",
        "\n",
        "    toHTML\n",
        "\n",
        "<PLANT>\n",
        "  <COMMON>Bloodroot</COMMON>\n",
        "  <BOTANICAL>Sanguinaria canadensis</BOTANICAL>\n",
        "  <ZONE>4</ZONE>\n",
        "  <LIGHT>Mostly Shady</LIGHT>\n",
        "  <PRICE>$2.44</PRICE>\n",
        "  <AVAILABILITY>031599</AVAILABILITY>\n",
        "</PLANT> \n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Loading raw data into data frames"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataframe column labels are the tags of the XML nodes, contained in the plant XML nodes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "column_names = names(plant_nodes[[1]])\n",
      "cat(column_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "COMMON BOTANICAL ZONE LIGHT PRICE AVAILABILITY"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a column name, we create a data frame column containing the column values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "common_tag = column_names[1]\n",
      "\n",
      "get_value = function(plant_node, tag) xmlValue(plant_node[[tag]])\n",
      "    \n",
      "common_column = sapply(plant_nodes, get_value, common_tag)\n",
      "    \n",
      "cat(common_column)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "Bloodroot Columbine Marsh Marigold Cowslip Dutchman's-Breeches Ginger, Wild Hepatica Liverleaf Jack-In-The-Pulpit Mayapple Phlox, Woodland Phlox, Blue Spring-Beauty Trillium Wake Robin Violet, Dog-Tooth Trout Lily Adder's-Tongue Anemone Grecian Windflower Bee Balm Bergamot Black-Eyed Susan Buttercup Crowfoot Butterfly Weed Cinquefoil Primrose Gentian Blue Gentian Jacob's Ladder Greek Valerian California Poppy Shooting Star Snakeroot Cardinal Flower"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We package the code above into a function that creates the data frame column corresponding from a tag name and a list of plant nodes: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "get_column = function(tag, plants) sapply(plants, get_value, tag)\n",
      "    \n",
      "cat((get_column('COMMON', plant_nodes)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "Bloodroot Columbine Marsh Marigold Cowslip Dutchman's-Breeches Ginger, Wild Hepatica Liverleaf Jack-In-The-Pulpit Mayapple Phlox, Woodland Phlox, Blue Spring-Beauty Trillium Wake Robin Violet, Dog-Tooth Trout Lily Adder's-Tongue Anemone Grecian Windflower Bee Balm Bergamot Black-Eyed Susan Buttercup Crowfoot Butterfly Weed Cinquefoil Primrose Gentian Blue Gentian Jacob's Ladder Greek Valerian California Poppy Shooting Star Snakeroot Cardinal Flower"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now ready to retrieve all the columns of our data frame into a list of vectors, which we will use to construct our data frame:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "data = lapply(column_names, get_column, plant_nodes)\n",
      "\n",
      "plant_df = data.frame(data, stringsAsFactors = FALSE)\n",
      "\n",
      "print(head(plant_df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "               COMMON              BOTANICAL ZONE        LIGHT PRICE\n",
        "1           Bloodroot Sanguinaria canadensis    4 Mostly Shady $2.44\n",
        "2           Columbine   Aquilegia canadensis    3 Mostly Shady $9.37\n",
        "3      Marsh Marigold       Caltha palustris    4 Mostly Sunny $6.81\n",
        "4             Cowslip       Caltha palustris    4 Mostly Shady $9.90\n",
        "5 Dutchman's-Breeches    Dicentra cucullaria    3 Mostly Shady $6.44\n",
        "6        Ginger, Wild       Asarum canadense    3 Mostly Shady $9.03\n",
        "  AVAILABILITY\n",
        "1       031599\n",
        "2       030699\n",
        "3       051799\n",
        "4       030699\n",
        "5       012099\n",
        "6       041899\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the data frame we just created, we save the plant data into a csv file into the raw data directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "write.csv(plant_df, './data/raw/plants_to_be_cleaned.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head ./data/raw/plants_to_be_cleaned.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Packaging the gathering code into reusable functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ./script/plant_df-R\n",
      "\n",
      "get_value  = function(plant_node, tag) xmlValue(plant_node[[tag]])\n",
      "    \n",
      "get_column = function(tag, plants) sapply(plants, get_value, tag)\n",
      "    \n",
      "create_df_from_plant_xml = function(plant_file){\n",
      "    require(XML)\n",
      "    xml_doc      = xmlParse(plant_file)\n",
      "    root_node    = xmlRoot(xml_doc)\n",
      "    plant_nodes  = xmlChildren(root_node)\n",
      "    column_names = names(plant_nodes[[1]])\n",
      "\n",
      "    data     = lapply(column_names, get_column, plant_nodes)\n",
      "    return(data.frame(data, stringsAsFactors = FALSE))\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ./script/plant_df-R\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, our function converting a raw plant XML file into a R data frame can be used in other notebooks, using the <code>source</code> command:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "\n",
      "source('./script/plant_df-R')\n",
      "\n",
      "print(head(create_df_from_plant_xml(plant_file)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "               COMMON              BOTANICAL ZONE        LIGHT PRICE\n",
        "1           Bloodroot Sanguinaria canadensis    4 Mostly Shady $2.44\n",
        "2           Columbine   Aquilegia canadensis    3 Mostly Shady $9.37\n",
        "3      Marsh Marigold       Caltha palustris    4 Mostly Sunny $6.81\n",
        "4             Cowslip       Caltha palustris    4 Mostly Shady $9.90\n",
        "5 Dutchman's-Breeches    Dicentra cucullaria    3 Mostly Shady $6.44\n",
        "6        Ginger, Wild       Asarum canadense    3 Mostly Shady $9.03\n",
        "  AVAILABILITY\n",
        "1       031599\n",
        "2       030699\n",
        "3       051799\n",
        "4       030699\n",
        "5       012099\n",
        "6       041899\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "____________________________________________\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Team members responsible for this notebook:\n",
      "\n",
      "* Anshuman Dewangan: Researched how all data (stock prices and Google Trends) should be gathered. Wrote all code for data gathering.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating the Directory Structure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "mkdir script data data/raw data/cleaned data/simulated visualizations\n",
      "ls -r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "visualizations\n",
        "script\n",
        "gtrends\n",
        "desktop.ini\n",
        "data\n",
        "cleaned\n",
        "SP500.csv\n",
        "README.md\n",
        "NB4_project_report.ipynb\n",
        "NB3_data_analysis.ipynb\n",
        "NB2_data_cleaning.ipynb\n",
        "NB1_data_gathering.ipynb\n",
        "DowJones.csv\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "mkdir: cannot create directory `script': File exists\n",
        "mkdir: cannot create directory `data': File exists\n",
        "mkdir: cannot create directory `data/raw': File exists\n",
        "mkdir: cannot create directory `data/cleaned': File exists\n",
        "mkdir: cannot create directory `data/simulated': File exists\n",
        "mkdir: cannot create directory `visualizations': File exists\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Gathering Code\n",
      "We will create a python file that downloads Google Trends data for different search terms related to the stocks in the Dow Jones Index. Then, we will add the stock prices to the Google Trends data and save the data in a csv file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's make all the necessary imports we will need for the program:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cookielib\n",
      "import csv\n",
      "import mechanize\n",
      "import re\n",
      "from StringIO import StringIO\n",
      "import sys\n",
      "import datetime\n",
      "import pandas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's load the file DowJones.csv, which contains all the stocks and ticker symbols in the Dow Jones Index, into a pandas dataframe. The csv file was manually created from a list of Dow Jones companies from Yahoo Finance.\n",
      "\n",
      "Source: http://finance.yahoo.com/q/cp?s=%5EDJI"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stocks = pandas.read_csv('./data/DowJones.csv')\n",
      "\n",
      "print stocks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                        Company Ticker\n",
        "0                      American Express Company    AXP\n",
        "1                            The Boeing Company     BA\n",
        "2                              Caterpillar Inc.    CAT\n",
        "3                           Cisco Systems, Inc.   CSCO\n",
        "4                           Chevron Corporation    CVX\n",
        "5          E. I. du Pont de Nemours and Company     DD\n",
        "6                       The Walt Disney Company    DIS\n",
        "7                      General Electric Company     GE\n",
        "8                 The Goldman Sachs Group, Inc.     GS\n",
        "9                          The Home Depot, Inc.     HD\n",
        "10  International Business Machines Corporation    IBM\n",
        "11                            Intel Corporation   INTC\n",
        "12                            Johnson & Johnson    JNJ\n",
        "13                         JPMorgan Chase & Co.    JPM\n",
        "14                        The Coca-Cola Company     KO\n",
        "15                             McDonald's Corp.    MCD\n",
        "16                                   3M Company    MMM\n",
        "17                             Merck & Co. Inc.    MRK\n",
        "18                        Microsoft Corporation   MSFT\n",
        "19                                   Nike, Inc.    NKE\n",
        "20                                  Pfizer Inc.    PFE\n",
        "21                 The Procter & Gamble Company     PG\n",
        "22                                   AT&T, Inc.      T\n",
        "23                The Travelers Companies, Inc.    TRV\n",
        "24              UnitedHealth Group Incorporated    UNH\n",
        "25                    United Technologies Corp.    UTX\n",
        "26                                    Visa Inc.      V\n",
        "27                  Verizon Communications Inc.     VZ\n",
        "28                         Wal-Mart Stores Inc.    WMT\n",
        "29                      Exxon Mobil Corporation    XOM\n",
        "\n",
        "[30 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results will be stored in a dictionary called \"stocks\" which creates key-value pairs for company-ticker."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creates a dictionary of company: ticker pairs and deletes header\n",
      "#stocks = {row[0]: row[1] for row in reader}\n",
      "#del stocks[\"Company\"]\n",
      "\n",
      "#print stocks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will create a list with all the queries we want to search on Google Trends. We want to test four different queries for each company:\n",
      "\n",
      "1. Company name\n",
      "2. Ticker symbol\n",
      "3. \"Buy\" + ticker symbol\n",
      "4. \"Sell\" + ticker symbol"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Creates a list with company names as the query\n",
      "companies = [stocks.Company[n] for n in range(0, len(stocks))]\n",
      "print companies[0:6]\n",
      "\n",
      "# Creates a list with tickr symbols as the query\n",
      "tickers = [stocks.Ticker[n] for n in range(0, len(stocks))]\n",
      "print tickers[0:6]\n",
      "\n",
      "# Creates a list with \"buy\" + ticker symbol as the query\n",
      "buy_co = [\"buy \" + stocks.Ticker[n] for n in range(0, len(stocks))]\n",
      "print buy_co[0:6]\n",
      "\n",
      "# Creates a list with \"sell\" + ticker symbol as the query\n",
      "sell_co = [\"sell \" + stocks.Ticker[n] for n in range(0, len(stocks))]\n",
      "print sell_co[0:6]\n",
      "\n",
      "# Adds all the queries into a list\n",
      "queries = [companies, tickers, buy_co, sell_co]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['American Express Company', 'The Boeing Company', 'Caterpillar Inc.', 'Cisco Systems, Inc.', 'Chevron Corporation', 'E. I. du Pont de Nemours and Company']\n",
        "['AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DD']\n",
        "['buy AXP', 'buy BA', 'buy CAT', 'buy CSCO', 'buy CVX', 'buy DD']\n",
        "['sell AXP', 'sell BA', 'sell CAT', 'sell CSCO', 'sell CVX', 'sell DD']\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we actually pull the data from Google Trends, we run into two problems:\n",
      "1. The data is only available to Google users\n",
      "2. The data is not directly downloadable from a csv url\n",
      "\n",
      "Therefore, we have create a virtual browser that would allow us to pull csv files directly by authenticating Google using our login credentials. Otherwise, there is no direct way to download the csv data. This portion of the code is modified from the source.\n",
      "\n",
      "Source: https://gist.github.com/tlmaloney/5650699"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Open browser\n",
      "br = mechanize.Browser()\n",
      "\n",
      "# Make it seem like we are an actual person\n",
      "br.set_handle_robots(False)\n",
      "\n",
      "# Create cookie jar\n",
      "cj = cookielib.LWPCookieJar()\n",
      "br.set_cookiejar(cj)\n",
      "\n",
      "# Act like we're a real browser\n",
      "br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "\n",
      "# Login in to Google\n",
      "response = br.open('https://accounts.google.com/ServiceLogin?hl=en&continue=https://www.google.com/')\n",
      "forms = mechanize.ParseResponse(response)\n",
      "form = forms[0]\n",
      "form['Email'] = \"DherintoR\"\n",
      "form['Passwd'] = \"forthestars\"\n",
      "response = br.open(form.click())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try getting Google Trends data for one of our queries for Johnson & Johnson. First, we'll put our queries, which are often more than one word, into a form the browser will understand: with \"%20\" between each word. Then we will append the search to our Google Trends url to get a csv of the Google Trends data.\n",
      "\n",
      "Source: http://www.google.com/trends/explore"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split queries list into a format that can be entered into the url       \n",
      "query_split = queries[0][0].split()\n",
      "query_params = '%20'.join(query_split)\n",
      "\n",
      "# Get CSV from Google Trends   \n",
      "response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "reader = csv.reader(StringIO(response.read()))\n",
      "\n",
      "# Print first 10 lines of csv file\n",
      "print queries[0][0]\n",
      "for x in range(0,10):\n",
      "    print reader.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Johnson & Johnson\n",
        "['Web Search interest: johnson']\n",
        "['Worldwide; 2004 - present']\n",
        "[]\n",
        "['Interest over time']\n",
        "['Week', 'johnson']\n",
        "['2004-01-04 - 2004-01-10', '29']\n",
        "['2004-01-11 - 2004-01-17', '29']\n",
        "['2004-01-18 - 2004-01-24', '31']\n",
        "['2004-01-25 - 2004-01-31', '29']\n",
        "['2004-02-01 - 2004-02-07', '30']\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interesting, the csv file comes with a lot of junk information. The actual Google Trends data comes in an awkward format, with dates given as weekly ranges. Let's clean the data so that it takes the Friday for each week and puts all the information into two different lists."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reload the page\n",
      "response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "reader = csv.reader(StringIO(response.read()))\n",
      "\n",
      "# Create empty lists to be populated with date and trend values\n",
      "dates = []\n",
      "values = []\n",
      "\n",
      "# Populate lists with dates and Google Trends values in the correct formate.\n",
      "for row in reader:\n",
      "    # Looks for the right rows to get data from, ignoring the junk\n",
      "    try:\n",
      "        date, value = row\n",
      "    except ValueError:\n",
      "        continue\n",
      "    # Takes the date for the Friday of the week and puts it in the correct format\n",
      "    if re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', date):\n",
      "        dates.append(str((datetime.datetime.strptime(date[-10:], \"%Y-%m-%d\")-datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\"))) # Uses last date in time period\n",
      "        values.append(value)\n",
      "\n",
      "#View the lists\n",
      "print dates[0:6]\n",
      "print values[0:6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['2004-01-09', '2004-01-16', '2004-01-23', '2004-01-30', '2004-02-06', '2004-02-13']\n",
        "['29', '29', '31', '29', '30', '31']\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try getting stock data on one of the ticker symbols using the Yahoo Finance API. We'll load a csv with daily stock prices of Johnson & Johnson (JNJ) from January 1, 2004 (when Google Trends data begins) to present.\n",
      "\n",
      "Source: https://code.google.com/p/yahoo-finance-managed/wiki/csvHistQuotesDownload"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a url that acceses Yahoo Finance historical data from 01-01-2004 to present for tickers[0] = JNJ\n",
      "base_url = \"http://ichart.yahoo.com/table.csv?s=\"\n",
      "dt_url = '%s&a=%s&b=%s&c=%s&d=%s&e=%s&f=%s&g=d&ignore=.csv'% (tickers[0], \"01\", \"01\", \"2004\", \"00\", \"00\", \"0000\")\n",
      "\n",
      "# Opens the csv file\n",
      "response1 = br.open(base_url + dt_url)\n",
      "reader1 = csv.reader(StringIO(response1.read()))\n",
      "\n",
      "# Print first 6 lines of csv file\n",
      "print tickers[0]\n",
      "for x in range(0,6):\n",
      "    print reader1.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "JNJ\n",
        "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "['2014-04-28', '100.56', '101.50', '100.12', '101.34', '8792900', '101.34']\n",
        "['2014-04-25', '99.97', '100.26', '99.40', '99.79', '5895900', '99.79']\n",
        "['2014-04-24', '100.24', '100.37', '99.55', '99.96', '6149900', '99.96']\n",
        "['2014-04-23', '100.24', '100.56', '99.92', '100.22', '5876000', '100.22']\n",
        "['2014-04-22', '100.00', '100.75', '99.74', '100.18', '7937900', '100.18']\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll save the closing prices of the stock data in a dictionary called \"prices_dict.\" We will use this dictionary to access specific closing prices later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reloads the csv file\n",
      "response1 = br.open(base_url + dt_url)\n",
      "reader1 = csv.reader(StringIO(response1.read()))\n",
      "\n",
      "# Creates a dictionary that maps dates to closing prices for that day\n",
      "prices_dict = {row[0]: row[4] for row in reader1}\n",
      "\n",
      "# Adds stock prices to the corresponding dates found in the Google Trends data\n",
      "prices = []\n",
      "for date in dates:\n",
      "    # Tries to add the price for the date from the Google Trends data\n",
      "    try:\n",
      "        prices.append(prices_dict[date])\n",
      "    # Adds a blank value if the price is not available for the Google Trends date\n",
      "    except:\n",
      "        prices.append(\"\")\n",
      "        \n",
      "# View the prices list\n",
      "print prices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['', '', '', '', '54.15', '54.23', '53.30', '53.91', '53.13', '50.71', '50.05', '50.13', '51.18', '', '54.13', '53.68', '54.03', '55.30', '54.52', '54.94', '55.71', '56.38', '', '55.25', '54.50', '55.35', '54.99', '56.80', '55.74', '55.27', '54.60', '55.54', '57.04', '57.71', '57.85', '57.57', '58.52', '56.55', '57.00', '55.32', '56.58', '57.29', '58.38', '59.18', '61.00', '60.54', '60.45', '61.91', '60.25', '63.58', '', '63.42', '62.61', '62.70', '61.85', '64.62', '66.24', '66.60', '65.43', '66.22', '67.74', '67.60', '67.25', '', '66.85', '68.64', '69.40', '68.49', '68.63', '68.21', '67.10', '67.20', '67.43', '66.44', '66.48', '66.56', '65.58', '64.95', '64.28', '65.03', '64.32', '63.96', '63.54', '63.56', '63.57', '61.94', '62.90', '64.73', '65.18', '63.99', '63.28', '61.34', '63.70', '64.07', '62.95', '60.88', '60.92', '62.55', '62.15', '61.21', '60.10', '60.86', '61.11', '60.10', '62.60', '61.82', '60.80', '58.71', '57.38', '58.39', '59.07', '57.76', '57.47', '59.04', '60.34', '60.39', '59.22', '57.83', '', '58.37', '58.61', '58.70', '58.83', '59.90', '60.69', '60.74', '61.38', '61.68', '61.32', '59.92', '60.62', '60.46', '61.73', '62.92', '63.53', '63.47', '64.42', '64.67', '64.72', '63.59', '63.79', '63.96', '64.94', '65.06', '64.58', '68.62', '68.17', '67.81', '66.29', '67.24', '65.85', '65.97', '65.95', '66.29', '65.65', '66.02', '66.62', '66.64', '67.76', '66.07', '66.58', '65.60', '65.51', '64.15', '61.95', '62.14', '60.51', '60.51', '60.26', '', '62.35', '65.12', '64.17', '64.48', '62.27', '63.42', '63.19', '63.41', '62.13', '62.77', '60.73', '61.62', '62.13', '63.43', '61.79', '59.77', '60.55', '61.15', '62.02', '61.88', '61.79', '61.68', '63.20', '65.12', '65.70', '66.25', '65.94', '64.23', '64.30', '64.78', '65.16', '67.75', '66.88', '67.74', '67.68', '67.59', '68.03', '67.38', '65.84', '67.88', '66.29', '62.46', '63.36', '62.03', '62.90', '63.18', '61.96', '61.51', '62.65', '', '64.18', '65.73', '66.00', '66.51', '67.31', '68.26', '66.55', '66.68', '64.92', '66.74', '65.76', '66.27', '64.06', '63.57', '', '66.26', '67.82', '69.03', '68.10', '71.55', '71.33', '71.42', '70.43', '70.67', '70.59', '69.99', '69.40', '66.16', '55.85', '62.65', '60.79', '61.34', '60.22', '60.05', '58.35', '58.58', '58.23', '57.25', '58.84', '58.56', '60.65', '59.05', '57.44', '55.97', '57.69', '58.51', '57.10', '54.65', '50.00', '47.97', '50.64', '51.67', '52.83', '52.15', '', '53.05', '50.92', '52.59', '54.98', '55.41', '54.77', '55.16', '55.93', '56.06', '56.09', '56.60', '', '56.93', '59.23', '61.51', '60.89', '59.90', '60.08', '61.03', '60.29', '60.32', '60.42', '60.78', '60.62', '59.73', '61.74', '60.46', '60.54', '59.05', '60.30', '61.43', '62.31', '62.89', '64.36', '64.85', '64.37', '', '', '64.21', '64.56', '63.20', '62.86', '62.64', '62.72', '63.81', '63.00', '64.04', '64.18', '65.11', '64.38', '', '65.14', '65.02', '65.04', '64.30', '63.31', '63.97', '60.88', '58.30', '58.01', '58.46', '59.18', '58.70', '59.08', '60.54', '59.44', '57.63', '58.09', '59.96', '58.15', '58.74', '57.60', '58.93', '59.98', '61.57', '62.14', '61.75', '63.23', '63.57', '63.81', '63.74', '64.65', '63.67', '63.83', '62.30', '62.56', '61.91', '62.54', '', '61.85', '62.60', '62.55', '62.66', '60.01', '60.84', '60.70', '61.11', '59.64', '61.06', '59.69', '58.57', '58.98', '59.49', '59.46', '60.56', '', '65.72', '65.27', '66.62', '65.69', '66.77', '66.09', '66.09', '66.29', '65.06', '67.30', '67.57', '67.45', '66.72', '64.79', '62.71', '63.36', '63.14', '64.28', '64.07', '63.64', '64.59', '61.59', '63.69', '63.13', '64.72', '63.78', '65.60', '64.12', '65.25', '63.85', '61.27', '63.47', '64.53', '64.30', '65.98', '65.58', '64.83', '65.26', '65.27', '65.56', '65.64', '64.60', '64.99', '64.46', '64.77', '64.74', '65.12', '64.55', '65.96', '', '63.54', '63.71', '64.84', '64.74', '64.34', '63.35', '62.51', '61.78', '62.98', '66.01', '66.63', '67.56', '67.64', '68.61', '68.63', '69.52', '69.12', '68.64', '67.80', '67.60', '67.43', '67.88', '68.47', '69.06', '68.91', '69.65', '67.97', '71.86', '70.90', '70.90', '69.87', '69.19', '69.56', '69.73', '70.45', '70.69', '70.27', '69.48', '71.55', '72.35', '73.23', '73.92', '74.18', '75.48', '76.16', '76.25', '76.70', '78.19', '79.19', '79.74', '', '82.04', '82.74', '84.49', '85.12', '85.75', '85.76', '88.09', '86.82', '84.18', '84.91', '84.91', '83.20', '85.86', '87.87', '89.99', '92.23', '92.83', '94.39', '92.36', '89.37', '88.41', '86.41', '87.16', '88.57', '89.68', '86.73', '87.31', '89.45', '91.63', '92.09', '93.37', '94.05', '94.39', '95.25', '94.66', '94.44', '91.35', '92.09', '92.35', '91.85', '94.74', '95.06', '90.61', '88.47', '90.04', '92.76', '91.52', '92.12', '93.32', '92.81', '95.93', '97.44', '98.42', '96.87', '', '99.79', '']\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great! Now we have three lists:\n",
      "1. Dates\n",
      "2. Google Trends values\n",
      "3. Stock prices\n",
      "The last step is to write a csv file containing all these values in separate columns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets the filename as the query\n",
      "filename = queries[0][0]\n",
      "filename.replace(\" \", \"\")\n",
      "\n",
      "# Creates a file and writes \n",
      "with open(\"data/raw/\" + filename + \"_gtrends.csv\", 'w') as f:\n",
      "    writer = csv.writer(f, lineterminator = '\\n')\n",
      "    # Writes a header row with the titles: Date, Query, Price\n",
      "    writer.writerow(['Date', queries[0][0], \"Price\"])\n",
      "    # Writes a row for each value of date, value, and price\n",
      "    for row in zip(dates, values, prices):\n",
      "        writer.writerow(row)\n",
      "    # Closes and saves the file\n",
      "    f.close()\n",
      "    \n",
      "# View contents of file\n",
      "with open(\"data/raw/\" + filename + \"_gtrends.csv\", 'r') as f:\n",
      "    for x in range(0,10):\n",
      "        print f.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Date,Johnson & Johnson,Price\n",
        "\n",
        "2004-01-09,29,\n",
        "\n",
        "2004-01-16,29,\n",
        "\n",
        "2004-01-23,31,\n",
        "\n",
        "2004-01-30,29,\n",
        "\n",
        "2004-02-06,30,54.15\n",
        "\n",
        "2004-02-13,31,54.23\n",
        "\n",
        "2004-02-20,31,53.30\n",
        "\n",
        "2004-02-27,32,53.91\n",
        "\n",
        "2004-03-05,29,53.13\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Actual Code Starts Here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Awesome! Now we can write a full script \"data_gathering.py\" that automates the process for all search queries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file script/data_gathering.py\n",
      "\n",
      "\"\"\"\n",
      "Downloads and cleans up CSV files from Google Trends of the stocks in Dow Jones Index and finds the stock price.\n",
      " \n",
      " \n",
      "Requires mechanize:\n",
      "    pip install mechanize\n",
      "\"\"\"\n",
      "import cookielib\n",
      "import csv\n",
      "import mechanize\n",
      "import re\n",
      "from StringIO import StringIO\n",
      "import sys\n",
      "import datetime\n",
      " \n",
      " \n",
      "# Pulls Yahoo Finance stock data for given ticker from 2004-present\n",
      "def make_url(ticker_symbol):\n",
      "    base_url = \"http://ichart.yahoo.com/table.csv?s=\"\n",
      "    dt_url = '%s&a=%s&b=%s&c=%s&d=%s&e=%s&f=%s&g=d&ignore=.csv'% (ticker_symbol, \"01\", \"01\", \"2004\", \"00\", \"00\", \"0000\")\n",
      "    return base_url + dt_url\n",
      "\n",
      "def browser_setup():\n",
      "    # Open browser\n",
      "    br = mechanize.Browser()\n",
      "    br.set_handle_robots(False)\n",
      "    \n",
      "    # Create cookie jar\n",
      "    cj = cookielib.LWPCookieJar()\n",
      "    br.set_cookiejar(cj)\n",
      "    \n",
      "    # Act like we're a real browser\n",
      "    br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]\n",
      "    \n",
      "    # Login in to Google\n",
      "    response = br.open('https://accounts.google.com/ServiceLogin?hl=en&continue=https://www.google.com/')\n",
      "    forms = mechanize.ParseResponse(response)\n",
      "    form = forms[0]\n",
      "    form['Email'] = \"DherintoR\"\n",
      "    form['Passwd'] = \"forthestars\"\n",
      "    response = br.open(form.click())\n",
      "\n",
      "# Opens csv file of DowJones.csv stocks and ticker symbols\n",
      "stocks_csv  = open('DowJones.csv', \"rb\")\n",
      "reader = csv.reader(stocks_csv)\n",
      "\n",
      "# Creates a dictionary of company: ticker pairs and deletes header\n",
      "stocks = {row[0]: row[1] for row in reader}\n",
      "del stocks[\"Company\"]\n",
      "\n",
      "# Creates the stocks list as the ticker symbol for each stock\n",
      "companies = [stocks.keys()[n] for n in range(0, len(stocks))]\n",
      "tickers = [stocks.values()[n] for n in range(0, len(stocks))]\n",
      "buy_co = [\"buy \" + stocks.values()[n] for n in range(0, len(stocks))]\n",
      "sell_co = [\"sell \" + stocks.values()[n] for n in range(0, len(stocks))]\n",
      "queries = [companies, tickers, buy_co, sell_co]\n",
      "\n",
      "#Get CSV for each stock in stocks list\n",
      "for n in range(0,len(companies)):\n",
      "    \n",
      "    #Get stock data for the stock\n",
      "    u =  make_url(tickers[n],\"0000-00-00\")\n",
      "    response1 = br.open(u)\n",
      "    reader1 = csv.reader(StringIO(response1.read()))\n",
      "    prices_dict = {row[0]: row[4] for row in reader1}\n",
      "\n",
      "    for x in range(0,len(queries)):\n",
      "        \n",
      "        # Get CSV from Google Trends          \n",
      "        query_split = queries[x][n].split()\n",
      "        query_params = '%20'.join(query_split)\n",
      "        response = br.open('http://www.google.com/trends/trendsReport?hl=en-US&q=' + query_params + '&cmpt=q&content=1&export=1') \n",
      "        reader = csv.reader(StringIO(response.read()))\n",
      "        \n",
      "        # Creates empty lists to be populated with data\n",
      "        dates = []\n",
      "        values = []\n",
      "        prices = []\n",
      "        \n",
      "        # Adds Google Trends dates and values\n",
      "        for row in reader:\n",
      "            try:\n",
      "                date, value = row\n",
      "            except ValueError:\n",
      "                continue\n",
      "            if re.search('[0-9]{4}-[0-9]{2}-[0-9]{2}', date):\n",
      "                dates.append(str((datetime.datetime.strptime(date[-10:], \"%Y-%m-%d\")-datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\"))) # Uses last date in time period\n",
      "                values.append(value)\n",
      "        \n",
      "        # Adds stock data for dates in Google Trends data\n",
      "        for date in dates:\n",
      "            try:\n",
      "                prices.append(prices_dict[date])\n",
      "            except:\n",
      "                prices.append(\"\")\n",
      "\n",
      "        #Creates a file names query_gtrends.csv with stored values in gtrends directory\n",
      "        filename = queries[x][n]\n",
      "        filename.replace(\" \", \"\")\n",
      "        with open(\"gtrends/\" + filename + \"_gtrends.csv\", 'w') as f:\n",
      "            writer = csv.writer(f, lineterminator = '\\n')\n",
      "            writer.writerow(['Date', queries[x][n], \"Price\"])\n",
      "            for row in zip(dates, values, prices):\n",
      "                writer.writerow(row)\n",
      "            f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting script/data_gathering.py\n"
       ]
      }
     ],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}